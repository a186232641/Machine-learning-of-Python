# 说在前面

假如输入的x数量为1000特征为100

那么原始形式是针对特征来求解参数的

对偶形式通过样本数量来求解的

不知道对不对🤷

# 感知机的原始形式

感知机的定义：知机是一个线性分类器，它的目的是找到一个能够将两类数据分开的线性边界。这个边界可以表示为权重向量$w$和偏置项$b$的线性组合。

模型定义：$f(x)=sign(w·x+b) $，其中$w\in\mathbf{R}^n$  $n$代表$n$维空间

$\left.sign=\left\{\begin{aligned}&+1,&&\mathrm{if~}x\geq0\\&-1,&&\mathrm{x<1}\end{aligned}\right.\right.$

点到平面的距离：

$d=\frac{|Ax_1+By_1+Cz_1+D|}{\sqrt{A^2+B^2+C^2}}$

空间中$R^n$任意一点到超平面$S$的距离为

$\frac1{||w||}|w\cdot x_0+b|$

$\text{||w||}$是L2范数相当于$\text{||w||}=\sqrt{w_1^2+w_2^2+w_3^2}$对应w属于多少维空间

对于误分类点：$-y_i(w\cdot x_i+b)>0$ 成立

因为当$w·x_i+b>0$时$y_i=-1$而当$w·x_i+b<0$时$y_i=+1$因此误分类点到超平面的距离为：$-\frac1{||w||}y_i(w\cdot x_i+b)$

假设总的误分类点的个数为$M$，那么所有误分类点到超平面$S$的总距离为

$-\frac1{||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b)$

给定训练数据集

​				$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

其中$x\in R^n$，$y\in{+1,-1}$

因此感知机的损失函数定义为：$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$

给定训练数据集

​				$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$

其中$x\in R^n$，$y\in{+1,-1}$，求参数$w、b$，使其为以下损失函数极小化问题的解

因此感知机的原始形式定义为：

$\min_{w,b}~L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$

其中M为误分类点的集合。

假设误分类点集合M是固定的，那么损失函数${L(w,b)}$的梯度由$\begin{aligned}&\bigtriangledown_wL(w,b)=-\sum_{x_i\in M}y_ix_i\\&\bigtriangledown_bL(w,b)=-\sum_{x_i\in M}y_i\end{aligned}$

对于误分类点$(x_i,y_i)$$w、b$的更新公式如下：

$\begin{aligned}&w\leftarrow w+\eta y_ix_i\\&b\leftarrow b+\eta y_i\end{aligned}$

```
import numpy as np

# 感知机的原始形式
class Perceptron:
    def __init__(self, learning_rate=1.0):
        self.lr = learning_rate
        self.weights = None
        self.bias = None

    def fit(self, X, y, epochs=1000):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(epochs):
            for idx, x_i in enumerate(X):
                linear_output = np.dot(x_i, self.weights) + self.bias
                y_predicted = np.sign(linear_output)
                if y[idx] * y_predicted <= 0:
                    self.weights += self.lr * y[idx] * x_i
                    self.bias += self.lr * y[idx]

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.sign(linear_output)

# 示例数据
X = np.array([[2, 3], [1, 5], [4, 2]]) # 特征
y = np.array([1, -1, 1])              # 标签

# 模型训练
perceptron = Perceptron()
perceptron.fit(X, y)

# 预测
predictions = perceptron.predict(X)
print("Predictions:", predictions)

```



# 感知机的对偶形式

对偶形式的基本想法是将w和b表示为**实例xi和标记yi的线性组合**，通过求解其系数而求得的w和b

$\begin{aligned}&w\leftarrow w+\eta y_ix_i\\&b\leftarrow b+\eta y_i\end{aligned}$

$\text{逐步修改}\mathrm{w}\text{、b,设修改n次,则}\mathrm{w},\mathrm{~b}\text{关于}(\mathrm{xi},\mathrm{yi})\text{的增量分别是}\mathrm{ai}\cdot\mathrm{yi}\cdot\mathrm{xi}\text{和}\mathrm{ai}\cdot\mathrm{yi},\text{ 这里}$

$\alpha_i=n_i\eta $

其中，$n_i$是每个点的累计修改次数，所以每个点的修改次数加起来就是总的修改次数$n$，即$n_i$满足

$\sum_{i=1}^Nn_i=n$，其中，$n$为总的修改次数。

其中$w,b$最终的可以分别表示为：

$\begin{gathered}w=\sum_{i=1}^N\alpha_iy_ix_i\\b=\sum_{i=1}^N\alpha_iy_i\end{gathered}$

对偶形式的感知机模型为：

输入：**线性可分**的数据集

$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$其中$x\in R^n$，$y\in{+1-1}$

$f(x)=\mathrm{sign}(\sum_{j=1}^N\alpha_j\cdot y_j\cdot x_j\cdot x+b)$

(1) a=0,b=0

(2)在训练集中选取数据$(x_i,y_i)$

(3) $y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i+b)\leq0$

则				$\begin{aligned}\alpha_i&\leftarrow\alpha_i+\eta\\b&\leftarrow b+\eta y_i\end{aligned}$

先说一个例子然后在说gram矩阵

假设样本包含 2个特征和一个类标签

$\begin{aligned}1.&x_1=(2,3),y_1=+1\\2.&x_2=(1,5),y_2=-1\\3.&x_3=(4,2),y_3=+1\end{aligned}$

$\begin{aligned}1.&\textbf{初始化:令 }\alpha=\begin{pmatrix}0,0,0\end{pmatrix}&\text{(长度为样本数量,这里是3),初始偏置 }b=0\text{,学习率}\\\eta&=1.\end{aligned}$

2.迭代更新：

$f(x_1)=sign((0\cdot+1\cdot(2,3)\cdot(2,3)+0\cdot-1\cdot1,5)\cdot(2,3)+0\cdot+1\cdot(4,2)\cdot(2,3))+0)
$

$f(x_1)=0=sign(0)=0$不正确，因此我们更新$\alpha_{1}\text{ 和 }b$

$\begin{aligned}\bullet&\alpha_1\leftarrow0+1=1\\\bullet&b\leftarrow0+1\cdot+1=1\end{aligned}$

接着检查$x_2$,$x_3$

检查所有样本是否正确分类，如果不是那么重复第二个步骤

Gram矩阵

```
import numpy as np

# 定义样本点
x1 = np.array([2, 3])
x2 = np.array([1, 5])
x3 = np.array([4, 2])

# 计算Gram矩阵
gram_matrix = np.array([
    [np.dot(x1, x1), np.dot(x1, x2), np.dot(x1, x3)],
    [np.dot(x2, x1), np.dot(x2, x2), np.dot(x2, x3)],
    [np.dot(x3, x1), np.dot(x3, x2), np.dot(x3, x3)]
])
array([[13, 17, 14],
       [17, 26, 14],
       [14, 14, 20]])
```

通过计算gram矩阵因此感知机的对偶形式比原始形式有更快的计算速度

```
import numpy as np

# 感知机的对偶形式
class PerceptronDual:
    def __init__(self, learning_rate=1.0):
        self.lr = learning_rate
        self.alpha = None
        self.bias = None
        self.X_train = None
        self.y_train = None

    def fit(self, X, y, epochs=1000):
        n_samples, _ = X.shape
        self.alpha = np.zeros(n_samples)
        self.bias = 0
        self.X_train = X
        self.y_train = y
        gram_matrix = np.dot(X, X.T)

        for _ in range(epochs):
            for i in range(n_samples):
                temp_sum = sum(self.alpha[j] * self.y_train[j] * gram_matrix[j, i] for j in range(n_samples))
                if y[i] * (temp_sum + self.bias) <= 0:
                    self.alpha[i] += self.lr
                    self.bias += self.lr * y[i]

    def predict(self, X):
        predictions = []
        for x in X:
            temp_sum = sum(self.alpha[j] * self.y_train[j] * np.dot(x, self.X_train[j]) for j in range(len(self.alpha)))
            predictions.append(np.sign(temp_sum + self.bias))
        return np.array(predictions)

# 示例数据
X = np.array([[2, 3], [1, 5], [4, 2]]) # 特征
y = np.array([1, -1, 1])              # 标签

# 模型训练
perceptron_dual = PerceptronDual()
perceptron_dual.fit(X, y)

# 预测
predictions_dual = perceptron_dual.predict(X)
print("Predictions (Dual):", predictions_dual)

```

